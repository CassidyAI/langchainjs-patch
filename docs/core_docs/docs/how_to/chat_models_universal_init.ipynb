{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdf4f09-8125-4ed1-8063-6feed57da8a3",
   "metadata": {},
   "source": [
    "# How to init any model in one line\n",
    "\n",
    "Many LLM applications let end users specify what model provider and model they want the application to be powered by. This requires writing some logic to initialize different ChatModels based on some user configuration. The `initChatModel()` helper method makes it easy to initialize a number of different model integrations without having to worry about import paths and class names.\n",
    "\n",
    "```{=mdx}\n",
    ":::tip Supported models\n",
    "\n",
    "See the [initChatModel()](https://v02.api.js.langchain.com/functions/langchain_chat_models_base.initChatModel.html) API reference for a full list of supported integrations.\n",
    "\n",
    "Make sure you have the integration packages installed for any model providers you want to support. E.g. you should have `@langchain/openai` installed to init an OpenAI model.\n",
    "\n",
    ":::\n",
    "\n",
    ":::info Requires ``langchain >= 0.2.11``\n",
    "\n",
    "This functionality was added in `langchain v0.2.11`. Please make sure your package is up to date.\n",
    "\n",
    ":::\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c9f57-a796-45f8-b6f4-3efd3f361a9b",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e14913-803c-4382-9009-5c6af3d75d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o: I'm an AI created by OpenAI, and I don't have a personal name. You can call me Assistant! How can I help you today?\n",
      "\n",
      "Claude Opus: My name is Claude. It's nice to meet you!\n",
      "\n",
      "Gemini 1.5: I am a large language model, trained by Google. I do not have a name. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { initChatModel } from \"langchain/chat_models\";\n",
    "\n",
    "// Returns a @langchain/openai ChatOpenAI instance.\n",
    "const gpt4o = await initChatModel(\n",
    "    \"gpt-4o\",\n",
    "    {\n",
    "        modelProvider: \"openai\",\n",
    "        temperature: 0,\n",
    "    }\n",
    ")\n",
    "// Returns a @langchain/anthropic ChatAnthropic instance.\n",
    "const claudeOpus = await initChatModel(\n",
    "    \"claude-3-opus-20240229\",\n",
    "    {\n",
    "        modelProvider: \"anthropic\",\n",
    "        temperature: 0,\n",
    "    }\n",
    ")\n",
    "// Returns a @langchain/google-vertexai ChatVertexAI instance.\n",
    "const gemini15 = await initChatModel(\n",
    "    \"gemini-1.5-pro\",\n",
    "    {\n",
    "        modelProvider: \"google_vertexai\",\n",
    "        temperature: 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "// Since all model integrations implement the ChatModel interface, you can use them in the same way.\n",
    "console.log(\"GPT-4o: \" + (await gpt4o.invoke(\"what's your name\")).content + \"\\n\")\n",
    "console.log(\"Claude Opus: \" + (await claudeOpus.invoke(\"what's your name\")).content + \"\\n\")\n",
    "console.log(\"Gemini 1.5: \" + (await gemini15.invoke(\"what's your name\")).content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811f219-5e78-4b62-b495-915d52a22532",
   "metadata": {},
   "source": [
    "## Inferring model provider\n",
    "\n",
    "For common and distinct model names `initChatModel()` will attempt to infer the model provider. See the [API reference](https://v02.api.js.langchain.com/functions/langchain_chat_models_base.initChatModel.html) for a full list of inference behavior. E.g. any model that starts with `gpt-3...` or `gpt-4...` will be inferred as using model provider `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0378ccc6-95bc-4d50-be50-fccc193f0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "const gpt4o = await initChatModel(\"gpt-4o\", {\n",
    "  temperature: 0\n",
    "})\n",
    "const claudeOpus = await initChatModel(\"claude-3-opus-20240229\", {\n",
    "  temperature: 0\n",
    "})\n",
    "const gemini15 = await initChatModel(\"gemini-1.5-pro\", {\n",
    "  temperature: 0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a44db-c50d-4846-951d-0f1c9ba8bbaa",
   "metadata": {},
   "source": [
    "## Creating a configurable model\n",
    "\n",
    "You can also create a runtime-configurable model by specifying `configurableFields`. If you don't specify a `model` value, then \"model\" and \"modelProvider\" be configurable by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c037f27-12d7-4e83-811e-4245c0e3ba58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any other name you prefer! How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-5428ab5c-b5c0-46de-9946-5d4ca40dbdc8-0', usage_metadata={'input_tokens': 11, 'output_tokens': 37, 'total_tokens': 48})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const configurableModel = await initChatModel({ temperature: 0 });\n",
    "\n",
    "await configurableModel.invoke(\n",
    "    \"what's your name\", { configurable: { model: \"gpt-4o\" }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "321e3036-abd2-4e1f-bcc6-606efd036954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"My name is Claude. It's nice to meet you!\", response_metadata={'id': 'msg_012XvotUJ3kGLXJUWKBVxJUi', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-1ad1eefe-f1c6-4244-8bc6-90e2cb7ee554-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await configurableModel.invoke(\n",
    "    \"what's your name\", { configurable: { model: \"claude-3-5-sonnet-20240620\" }}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b3d4a-4066-45e4-8297-ea81ac8e70b7",
   "metadata": {},
   "source": [
    "### Configurable model with default values\n",
    "\n",
    "We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814a2289-d0db-401e-b555-d5116112b413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any other name you prefer! How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ce0793330f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3923e328-7715-4cd6-b215-98e4b6bf7c9d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 37, 'total_tokens': 48})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const firstLlm = await initChatModel(\n",
    "    \"gpt-4o\",\n",
    "    {\n",
    "        temperature: 0,\n",
    "        configurableFields: [\"model\", \"modelProvider\", \"temperature\", \"maxTokens\"],\n",
    "        configPrefix: \"first\" // useful when you have a chain with multiple models\n",
    "    }\n",
    ")\n",
    "\n",
    "await firstLlm.invoke(\"what's your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8755ba-c001-4f5a-a497-be3f1db83244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"My name is Claude. It's nice to meet you!\", response_metadata={'id': 'msg_01RyYR64DoMPNCfHeNnroMXm', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-22446159-3723-43e6-88df-b84797e7751d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await firstLlm.invoke(\n",
    "    \"what's your name\",\n",
    "    {\n",
    "        configurable: {\n",
    "            first_model: \"claude-3-5-sonnet-20240620\",\n",
    "            first_temperature: 0.5,\n",
    "            first_maxTokens: 100,\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072b1a3-7e44-4b4e-8b07-efe1ba91a689",
   "metadata": {},
   "source": [
    "### Using a configurable model declaratively\n",
    "\n",
    "We can call declarative operations like `bind_tools`, `with_structured_output`, `with_configurable`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067dabee-1050-4110-ae24-c48eba01e13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetPopulation',\n",
       "  'args': {'location': 'Los Angeles, CA'},\n",
       "  'id': 'call_sYT3PFMufHGWJD32Hi2CTNUP'},\n",
       " {'name': 'GetPopulation',\n",
       "  'args': {'location': 'New York, NY'},\n",
       "  'id': 'call_j1qjhxRnD3ffQmRyqjlI1Lnk'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "\n",
    "const GetWeather = z.object({\n",
    "    location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n",
    "}).describe(\"Get the current weather in a given location\");\n",
    "const weatherTool = tool((input) => {\n",
    "    // do something\n",
    "    return \"138 degrees\"\n",
    "}, {\n",
    "    name: \"GetWeather\",\n",
    "    schema: GetWeather\n",
    "})\n",
    "\n",
    "const GetPopulation = z.object({\n",
    "    location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n",
    "}).describe(\"Get the current population in a given location\");\n",
    "const populationTool = tool((input) => {\n",
    "    // do something\n",
    "    return \"one hundred billion\"\n",
    "}, {\n",
    "    name: \"GetPopulation\",\n",
    "    schema: GetPopulation\n",
    "})\n",
    "\n",
    "const llm = await initChatModel({ temperature: 0 })\n",
    "const llmWithTools = llm.bindTools([weatherTool, populationTool])\n",
    "\n",
    "(await llmWithTools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\", { configurable: { model: \"gpt-4o\" }}\n",
    ")).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57dfe9f-cd24-4e37-9ce9-ccf8daf78f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetPopulation',\n",
       "  'args': {'location': 'Los Angeles, CA'},\n",
       "  'id': 'toolu_01CxEHxKtVbLBrvzFS7GQ5xR'},\n",
       " {'name': 'GetPopulation',\n",
       "  'args': {'location': 'New York City, NY'},\n",
       "  'id': 'toolu_013A79qt5toWSsKunFBDZd5S'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await llmWithTools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\", { configurable: { model: \"claude-3-5-sonnet-20240620\" }}\n",
    ")).tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "typescript",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
